{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98242d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "#number of qubits and data dimension\n",
    "n_qubits = 3  \n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "#Step 1: Generate intermediate quantum state\n",
    "def generate_intermediate_state(x, w):\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    x = x / norm_x if norm_x != 0 else x\n",
    "    for i, val in enumerate(x):\n",
    "        qml.RX(val, wires=i)\n",
    "        qml.RZ(val, wires=i)  #feature encoding\n",
    "    for i, val in enumerate(w):\n",
    "        qml.RY(val, wires=i)\n",
    "    qml.CZ(wires=[0, 1])\n",
    "    qml.CZ(wires=[1, 2])  #entanglement\n",
    "\n",
    "\n",
    "#step 2: Quantum amplitude estimation for gradient computation\n",
    "@qml.qnode(dev)\n",
    "def compute_gradient_amplitude(x, weights):\n",
    "    quantum_feature_map(x)\n",
    "    for i in range(len(weights)):\n",
    "        qml.RY(weights[i], wires=i)  # First layer of learnable parameters\n",
    "    qml.CZ(wires=[0, 1])  #entanglement\n",
    "    for i in range(len(weights)):\n",
    "        qml.RY(weights[i] / 2, wires=i)  # Second layer of learnable parameters\n",
    "    return qml.probs(wires=range(len(x)))\n",
    "\n",
    "def quantum_feature_map(x):\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    x = x / norm_x if norm_x != 0 else x\n",
    "    for i, val in enumerate(x):\n",
    "        qml.RX(val, wires=i)\n",
    "        qml.RZ(val, wires=i)  # Add non-linear feature encoding\n",
    "    qml.CZ(wires=[0, 1])  # Add entanglement\n",
    "    qml.CZ(wires=[1, 2])  # Additional entanglement\n",
    "\n",
    "\n",
    "#step 3: Classical gradient computation\n",
    "def classical_gradient(x, y, w):\n",
    "    \"\"\"\n",
    "    Computes the classical gradient using quantum measurements.\n",
    "        x: Input data point\n",
    "        y: Ground truth label for the data point\n",
    "        w: Parameters\n",
    "    \"\"\"\n",
    "    amplitude_probs = compute_gradient_amplitude(x, w)\n",
    "    amplitude_probs = np.clip(amplitude_probs, 1e-6, 1 - 1e-6)  #clip probabilities\n",
    "    gradient = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        gradient[i] = (2 * amplitude_probs[i] - 1) * np.linalg.norm(x)\n",
    "    return gradient\n",
    "\n",
    "#training Loop\n",
    "def train_quantum_logistic_regression(X, y, w_init, lr, epsilon, max_steps):\n",
    "    w = w_init\n",
    "    for step in range(max_steps):\n",
    "        grad = np.zeros_like(w)\n",
    "        for i, x in enumerate(X):\n",
    "            gradient = classical_gradient(x, y[i], w)\n",
    "            grad += gradient\n",
    "        grad /= len(X)\n",
    "\n",
    "        #clip gradients for updates\n",
    "        grad = np.clip(grad, -1, 1)\n",
    "        \n",
    "        w = w - lr * grad\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            print(f\"Converged after {step} steps.\")\n",
    "            break\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce5209db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #regularized Logistic Loss Function\n",
    "\n",
    "def cost(weights, X, y):\n",
    "    predictions = np.array([compute_gradient_amplitude(x, weights)[0] for x in X])\n",
    "    predictions = np.clip(predictions, 1e-6, 1 - 1e-6) \n",
    "    loss = np.mean(-y * np.log(predictions) - (1 - y) * np.log(1 - predictions))\n",
    "    regularization = 0.001 * np.sum(weights**2)  \n",
    "    return loss + regularization\n",
    "\n",
    "\n",
    "#gradient Descent with Cost Function Integration\n",
    "def train_quantum_logistic_regression_with_cost(X, y, w_init, lr, epsilon, max_steps):\n",
    "    \"\"\"\n",
    "    Train the quantum logistic regression model using the cost function.\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    for step in range(max_steps):\n",
    "        grad = np.zeros_like(w)\n",
    "        for i, x in enumerate(X):\n",
    "            gradient = classical_gradient(x, y[i], w)\n",
    "            grad += gradient\n",
    "        grad /= len(X)\n",
    "        \n",
    "        # Compute the current cost for monitoring\n",
    "        current_cost = cost(w, X, y)\n",
    "        print(f\"Step {step}, Cost: {current_cost}\")\n",
    "        \n",
    "        # Update weights using gradient descent\n",
    "        grad = np.clip(grad, -1, 1)  # Gradient clipping for stability\n",
    "        w = w - lr * grad\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            print(f\"Converged after {step} steps.\")\n",
    "            break\n",
    "    \n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85670f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Cost: 1.6522732954608972\n",
      "Step 1, Cost: 1.6487358027192571\n",
      "Step 2, Cost: 1.6452179624750178\n",
      "Step 3, Cost: 1.6417196477209688\n",
      "Step 4, Cost: 1.6382407302779491\n",
      "Step 5, Cost: 1.6347810809439318\n",
      "Step 6, Cost: 1.6313405696344474\n",
      "Step 7, Cost: 1.6279190655146103\n",
      "Step 8, Cost: 1.624516437123205\n",
      "Step 9, Cost: 1.6211325524894\n",
      "Step 10, Cost: 1.617767279242349\n",
      "Step 11, Cost: 1.6144204847140353\n",
      "Step 12, Cost: 1.611092036035932\n",
      "Step 13, Cost: 1.6077818002295514\n",
      "Step 14, Cost: 1.6044896442914307\n",
      "Step 15, Cost: 1.6012154352725467\n",
      "Step 16, Cost: 1.5979590403529598\n",
      "Step 17, Cost: 1.5947203269113954\n",
      "Step 18, Cost: 1.5914991625904338\n",
      "Step 19, Cost: 1.5882954153573394\n",
      "Step 20, Cost: 1.5851089535608358\n",
      "Step 21, Cost: 1.5819396459841037\n",
      "Step 22, Cost: 1.5787873618940833\n",
      "Step 23, Cost: 1.575651971087344\n",
      "Step 24, Cost: 1.5725333439327573\n",
      "Step 25, Cost: 1.5694313514110674\n",
      "Step 26, Cost: 1.566345865151615\n",
      "Step 27, Cost: 1.5632767574662711\n",
      "Step 28, Cost: 1.56022390138079\n",
      "Step 29, Cost: 1.5571871706637632\n",
      "Step 30, Cost: 1.5541664398532775\n",
      "Step 31, Cost: 1.5511615842813242\n",
      "Step 32, Cost: 1.548172480096188\n",
      "Step 33, Cost: 1.5451990042829893\n",
      "Step 34, Cost: 1.5422410346822415\n",
      "Step 35, Cost: 1.539298450006815\n",
      "Step 36, Cost: 1.5363711298571912\n",
      "Step 37, Cost: 1.53345895473522\n",
      "Step 38, Cost: 1.5305618060564508\n",
      "Step 39, Cost: 1.5276795661610771\n",
      "Step 40, Cost: 1.5248121183235774\n",
      "Step 41, Cost: 1.5219593467611938\n",
      "Step 42, Cost: 1.5191211366412591\n",
      "Step 43, Cost: 1.5162973740874426\n",
      "Step 44, Cost: 1.5134879461849875\n",
      "Step 45, Cost: 1.510692740985028\n",
      "Step 46, Cost: 1.5079116475080008\n",
      "Step 47, Cost: 1.5051445557462133\n",
      "Step 48, Cost: 1.502391356665638\n",
      "Step 49, Cost: 1.4996519422069885\n",
      "Step 50, Cost: 1.496926205286078\n",
      "Step 51, Cost: 1.4942140397935066\n",
      "Step 52, Cost: 1.4915153405938668\n",
      "Step 53, Cost: 1.4888300035242106\n",
      "Step 54, Cost: 1.4861579253921091\n",
      "Step 55, Cost: 1.4834990039731624\n",
      "Step 56, Cost: 1.480853138008093\n",
      "Step 57, Cost: 1.4782202271993254\n",
      "Step 58, Cost: 1.4756001722072765\n",
      "Step 59, Cost: 1.4729928746461864\n",
      "Step 60, Cost: 1.4703982370796134\n",
      "Step 61, Cost: 1.46781616301563\n",
      "Step 62, Cost: 1.4652465569017328\n",
      "Step 63, Cost: 1.4626893241194123\n",
      "Step 64, Cost: 1.460144370978549\n",
      "Step 65, Cost: 1.4576116047115035\n",
      "Step 66, Cost: 1.455090933467023\n",
      "Step 67, Cost: 1.4525822663039447\n",
      "Step 68, Cost: 1.4500855131847228\n",
      "Step 69, Cost: 1.4476005849687525\n",
      "Step 70, Cost: 1.4451273934056181\n",
      "Step 71, Cost: 1.44266585112809\n",
      "Step 72, Cost: 1.440215871645117\n",
      "Step 73, Cost: 1.4377773693346219\n",
      "Step 74, Cost: 1.4353502594362264\n",
      "Step 75, Cost: 1.4329344580439067\n",
      "Step 76, Cost: 1.4305298820985106\n",
      "Step 77, Cost: 1.4281364493802808\n",
      "Step 78, Cost: 1.4257540785012774\n",
      "Step 79, Cost: 1.423382688897719\n",
      "Step 80, Cost: 1.4210222008223656\n",
      "Step 81, Cost: 1.4186725353367942\n",
      "Step 82, Cost: 1.4163336143036713\n",
      "Step 83, Cost: 1.4140053603790037\n",
      "Step 84, Cost: 1.4116876970043863\n",
      "Step 85, Cost: 1.4093805483992263\n",
      "Step 86, Cost: 1.4070838395529721\n",
      "Step 87, Cost: 1.4047974962173404\n",
      "Step 88, Cost: 1.402521444898562\n",
      "Step 89, Cost: 1.4002556128496189\n",
      "Step 90, Cost: 1.3979999280625472\n",
      "Step 91, Cost: 1.3957543192606834\n",
      "Step 92, Cost: 1.3935187158910167\n",
      "Step 93, Cost: 1.391293048116507\n",
      "Step 94, Cost: 1.3890772468084733\n",
      "Step 95, Cost: 1.3868712435390018\n",
      "Step 96, Cost: 1.3846749705734143\n",
      "Step 97, Cost: 1.3824883608627112\n",
      "Step 98, Cost: 1.3803113480361522\n",
      "Step 99, Cost: 1.3781438663937997\n",
      "Step 100, Cost: 1.3759858508991751\n",
      "Step 101, Cost: 1.373837237171908\n",
      "Step 102, Cost: 1.3716979614804703\n",
      "Step 103, Cost: 1.3695679607349645\n",
      "Step 104, Cost: 1.36744717247994\n",
      "Step 105, Cost: 1.3653355348873017\n",
      "Step 106, Cost: 1.3632329867492334\n",
      "Step 107, Cost: 1.3611394674712216\n",
      "Step 108, Cost: 1.3590549170651196\n",
      "Step 109, Cost: 1.3569792761422446\n",
      "Step 110, Cost: 1.3549124859066062\n",
      "Step 111, Cost: 1.3528544881481042\n",
      "Step 112, Cost: 1.3508052252358804\n",
      "Step 113, Cost: 1.348764640111662\n",
      "Step 114, Cost: 1.346732676283198\n",
      "Step 115, Cost: 1.3447092778177794\n",
      "Step 116, Cost: 1.3426943893357723\n",
      "Step 117, Cost: 1.3406879560042682\n",
      "Step 118, Cost: 1.338689923530774\n",
      "Step 119, Cost: 1.3367002381569457\n",
      "Step 120, Cost: 1.334718846652436\n",
      "Step 121, Cost: 1.332745696308772\n",
      "Step 122, Cost: 1.3307807349332994\n",
      "Step 123, Cost: 1.3288239108432023\n",
      "Step 124, Cost: 1.3268751728595847\n",
      "Step 125, Cost: 1.3249344703016228\n",
      "Step 126, Cost: 1.32300175298076\n",
      "Step 127, Cost: 1.321076971194995\n",
      "Step 128, Cost: 1.3191600757232098\n",
      "Step 129, Cost: 1.3172510178195846\n",
      "Step 130, Cost: 1.3153497492080275\n",
      "Step 131, Cost: 1.3134562220767674\n",
      "Step 132, Cost: 1.311570389072885\n",
      "Step 133, Cost: 1.309692203296998\n",
      "Step 134, Cost: 1.3078216182979598\n",
      "Step 135, Cost: 1.3059585880676612\n",
      "Step 136, Cost: 1.3041030670358442\n",
      "Step 137, Cost: 1.3022550100650172\n",
      "Step 138, Cost: 1.3004143724454178\n",
      "Step 139, Cost: 1.2985811098900213\n",
      "Step 140, Cost: 1.2967551785296236\n",
      "Step 141, Cost: 1.2949365349079969\n",
      "Step 142, Cost: 1.293125135977059\n",
      "Step 143, Cost: 1.2913209390921574\n",
      "Step 144, Cost: 1.2895239020073628\n",
      "Step 145, Cost: 1.2877339828708456\n",
      "Step 146, Cost: 1.2859511402203059\n",
      "Step 147, Cost: 1.2841753329784547\n",
      "Step 148, Cost: 1.2824065204485517\n",
      "Step 149, Cost: 1.2806446623099932\n",
      "Step 150, Cost: 1.278889718613958\n",
      "Step 151, Cost: 1.2771416497791261\n",
      "Step 152, Cost: 1.2754004165874124\n",
      "Step 153, Cost: 1.2736659801797858\n",
      "Step 154, Cost: 1.2719383020521202\n",
      "Step 155, Cost: 1.270217344051113\n",
      "Step 156, Cost: 1.2685030683702376\n",
      "Step 157, Cost: 1.2667954375457613\n",
      "Step 158, Cost: 1.265094414452806\n",
      "Step 159, Cost: 1.2633999623014525\n",
      "Step 160, Cost: 1.2617120446329184\n",
      "Step 161, Cost: 1.2600306253157405\n",
      "Step 162, Cost: 1.2583556685420503\n",
      "Step 163, Cost: 1.2566871388238665\n",
      "Step 164, Cost: 1.2550250009894555\n",
      "Step 165, Cost: 1.2533692201797049\n",
      "Step 166, Cost: 1.2517197618445843\n",
      "Step 167, Cost: 1.2500765917396268\n",
      "Step 168, Cost: 1.2484396759224459\n",
      "Step 169, Cost: 1.2468089807493263\n",
      "Step 170, Cost: 1.2451844728718282\n",
      "Step 171, Cost: 1.2435661192334524\n",
      "Step 172, Cost: 1.2419538870663502\n",
      "Step 173, Cost: 1.2403477438880306\n",
      "Step 174, Cost: 1.2387476574982077\n",
      "Step 175, Cost: 1.2371535959755622\n",
      "Step 176, Cost: 1.2355655276746518\n",
      "Step 177, Cost: 1.23398342122278\n",
      "Step 178, Cost: 1.2324072455169834\n",
      "Step 179, Cost: 1.2308369697209667\n",
      "Step 180, Cost: 1.2292725632621666\n",
      "Step 181, Cost: 1.2277139958287835\n",
      "Step 182, Cost: 1.226161237366877\n",
      "Step 183, Cost: 1.2246142580775248\n",
      "Step 184, Cost: 1.2230730284139506\n",
      "Step 185, Cost: 1.2215375190787754\n",
      "Step 186, Cost: 1.2200077010212134\n",
      "Step 187, Cost: 1.2184835454343805\n",
      "Step 188, Cost: 1.216965023752576\n",
      "Step 189, Cost: 1.215452107648652\n",
      "Step 190, Cost: 1.2139447690313734\n",
      "Step 191, Cost: 1.2124429800428242\n",
      "Step 192, Cost: 1.2109467130558704\n",
      "Step 193, Cost: 1.2094559406716026\n",
      "Step 194, Cost: 1.2079706357168785\n",
      "Step 195, Cost: 1.2064907712418362\n",
      "Step 196, Cost: 1.2050163205174607\n",
      "Step 197, Cost: 1.2035472570332009\n",
      "Step 198, Cost: 1.202083554494591\n",
      "Step 199, Cost: 1.2006251868208975\n",
      "Step 200, Cost: 1.1991721281428247\n",
      "Step 201, Cost: 1.1977243528002282\n",
      "Step 202, Cost: 1.1962818353398437\n",
      "Step 203, Cost: 1.1948445505130911\n",
      "Step 204, Cost: 1.1934124732738498\n",
      "Step 205, Cost: 1.1919855787762994\n",
      "Step 206, Cost: 1.1905638423727734\n",
      "Step 207, Cost: 1.1891472396116503\n",
      "Step 208, Cost: 1.1877357462352467\n",
      "Step 209, Cost: 1.1863293381777626\n",
      "Step 210, Cost: 1.184927991563251\n",
      "Step 211, Cost: 1.1835316827035833\n",
      "Step 212, Cost: 1.1821403880964834\n",
      "Step 213, Cost: 1.180754084423537\n",
      "Step 214, Cost: 1.179372748548281\n",
      "Step 215, Cost: 1.177996357514266\n",
      "Step 216, Cost: 1.1766248885431771\n",
      "Step 217, Cost: 1.1752583190329553\n",
      "Step 218, Cost: 1.1738966265559583\n",
      "Step 219, Cost: 1.1725397888571354\n",
      "Step 220, Cost: 1.1711877838522302\n",
      "Step 221, Cost: 1.1698405896259867\n",
      "Step 222, Cost: 1.168498184430423\n",
      "Step 223, Cost: 1.1671605466830608\n",
      "Step 224, Cost: 1.1658276549652375\n",
      "Step 225, Cost: 1.1644994880204071\n",
      "Step 226, Cost: 1.1631760247524536\n",
      "Step 227, Cost: 1.1618572442240633\n",
      "Step 228, Cost: 1.1605431256550691\n",
      "Step 229, Cost: 1.1592336484208632\n",
      "Step 230, Cost: 1.157928792050774\n",
      "Step 231, Cost: 1.156628536226527\n",
      "Step 232, Cost: 1.1553328607806637\n",
      "Step 233, Cost: 1.1540417456950218\n",
      "Step 234, Cost: 1.1527551710992134\n",
      "Step 235, Cost: 1.151473117269126\n",
      "Step 236, Cost: 1.1501955646254454\n",
      "Step 237, Cost: 1.1489224937321922\n",
      "Step 238, Cost: 1.1476538852952767\n",
      "Step 239, Cost: 1.1463897201610698\n",
      "Step 240, Cost: 1.1451299793149974\n",
      "Step 241, Cost: 1.143874643880142\n",
      "Step 242, Cost: 1.1426236951158706\n",
      "Step 243, Cost: 1.1413771144164684\n",
      "Step 244, Cost: 1.1401348833098062\n",
      "Step 245, Cost: 1.1388969834559974\n",
      "Step 246, Cost: 1.1376633966460976\n",
      "Step 247, Cost: 1.136434104800804\n",
      "Step 248, Cost: 1.1352090899691785\n",
      "Step 249, Cost: 1.1339883343273693\n",
      "Step 250, Cost: 1.132771820177376\n",
      "Step 251, Cost: 1.1315595299458006\n",
      "Step 252, Cost: 1.130351446182638\n",
      "Step 253, Cost: 1.1291475515600595\n",
      "Step 254, Cost: 1.12794782887122\n",
      "Step 255, Cost: 1.1267522610290868\n",
      "Step 256, Cost: 1.1255608310652678\n",
      "Step 257, Cost: 1.124373522128864\n",
      "Step 258, Cost: 1.1231903174853262\n",
      "Step 259, Cost: 1.1220112005153413\n",
      "Step 260, Cost: 1.1208361547137065\n",
      "Step 261, Cost: 1.119665163688244\n",
      "Step 262, Cost: 1.1184982111587054\n",
      "Step 263, Cost: 1.1173352809557056\n",
      "Step 264, Cost: 1.1161763570196641\n",
      "Step 265, Cost: 1.115021423399743\n",
      "Step 266, Cost: 1.1138704642528279\n",
      "Step 267, Cost: 1.1127234638424939\n",
      "Step 268, Cost: 1.1115804065379988\n",
      "Step 269, Cost: 1.1104412768132699\n",
      "Step 270, Cost: 1.1093060592459407\n",
      "Step 271, Cost: 1.1081747385163412\n",
      "Step 272, Cost: 1.107047299406558\n",
      "Step 273, Cost: 1.1059237267994637\n",
      "Step 274, Cost: 1.1048040056777741\n",
      "Step 275, Cost: 1.1036881211231298\n",
      "Step 276, Cost: 1.1025760583151472\n",
      "Step 277, Cost: 1.1014678025305247\n",
      "Step 278, Cost: 1.1003633391421395\n",
      "Step 279, Cost: 1.0992626536181471\n",
      "Step 280, Cost: 1.0981657315211084\n",
      "Step 281, Cost: 1.0970725585071117\n",
      "Step 282, Cost: 1.095983120324914\n",
      "Step 283, Cost: 1.0948974028150908\n",
      "Step 284, Cost: 1.0938153919091882\n",
      "Step 285, Cost: 1.0927370736288915\n",
      "Step 286, Cost: 1.0916624340852092\n",
      "Step 287, Cost: 1.0905914594776478\n",
      "Step 288, Cost: 1.0895241360934123\n",
      "Step 289, Cost: 1.0884604503066093\n",
      "Step 290, Cost: 1.0874003885774597\n",
      "Step 291, Cost: 1.0863439374515218\n",
      "Step 292, Cost: 1.0852910835589147\n",
      "Step 293, Cost: 1.0842418136135692\n",
      "Step 294, Cost: 1.0831961144124624\n",
      "Step 295, Cost: 1.0821539728348781\n",
      "Step 296, Cost: 1.0811153758416798\n",
      "Step 297, Cost: 1.080080310474563\n",
      "Step 298, Cost: 1.0790487638553554\n",
      "Step 299, Cost: 1.0780207231852914\n",
      "Step 300, Cost: 1.076996175744316\n",
      "Step 301, Cost: 1.0759751088903822\n",
      "Step 302, Cost: 1.0749575100587674\n",
      "Step 303, Cost: 1.073943366761389\n",
      "Step 304, Cost: 1.0729326665861303\n",
      "Step 305, Cost: 1.0719253971961789\n",
      "Step 306, Cost: 1.0709215463293587\n",
      "Step 307, Cost: 1.0699211017974903\n",
      "Step 308, Cost: 1.0689240514857368\n",
      "Step 309, Cost: 1.067930383351965\n",
      "Step 310, Cost: 1.0669400854261235\n",
      "Step 311, Cost: 1.0659531458096085\n",
      "Step 312, Cost: 1.0649695526746592\n",
      "Step 313, Cost: 1.0639892942637337\n",
      "Step 314, Cost: 1.0630123588889082\n",
      "Step 315, Cost: 1.0620387349312892\n",
      "Step 316, Cost: 1.0610684108404096\n",
      "Step 317, Cost: 1.0601013751336568\n",
      "Step 318, Cost: 1.0591376163956778\n",
      "Step 319, Cost: 1.058177123277826\n",
      "Step 320, Cost: 1.0572198844975769\n",
      "Step 321, Cost: 1.0562658888379879\n",
      "Step 322, Cost: 1.0553151251471165\n",
      "Step 323, Cost: 1.054367582337506\n",
      "Step 324, Cost: 1.0534232493856173\n",
      "Step 325, Cost: 1.0524821153313024\n",
      "Step 326, Cost: 1.0515441692772702\n",
      "Step 327, Cost: 1.050609400388565\n",
      "Step 328, Cost: 1.0496777978920437\n",
      "Step 329, Cost: 1.048749351075865\n",
      "Step 330, Cost: 1.0478240492889743\n",
      "Step 331, Cost: 1.0469018819406064\n",
      "Step 332, Cost: 1.0459828384997876\n",
      "Step 333, Cost: 1.0450669084948356\n",
      "Step 334, Cost: 1.044154081512879\n",
      "Step 335, Cost: 1.0432443471993713\n",
      "Step 336, Cost: 1.0423376952576122\n",
      "Step 337, Cost: 1.0414341154482818\n",
      "Step 338, Cost: 1.0405335975889614\n",
      "Step 339, Cost: 1.0396361315536815\n",
      "Step 340, Cost: 1.0387417072724556\n",
      "Step 341, Cost: 1.0378503147308347\n",
      "Step 342, Cost: 1.0369619439694515\n",
      "Step 343, Cost: 1.0360765850835811\n",
      "Step 344, Cost: 1.0351942282226954\n",
      "Step 345, Cost: 1.0343148635900379\n",
      "Step 346, Cost: 1.0334384814421873\n",
      "Step 347, Cost: 1.0325650720886264\n",
      "Step 348, Cost: 1.0316946258913369\n",
      "Step 349, Cost: 1.030827133264361\n",
      "Step 350, Cost: 1.0299625846734044\n",
      "Step 351, Cost: 1.029100970635422\n",
      "Step 352, Cost: 1.028242281718211\n",
      "Step 353, Cost: 1.0273865085400133\n",
      "Step 354, Cost: 1.02653364176912\n",
      "Step 355, Cost: 1.025683672123476\n",
      "Step 356, Cost: 1.0248365903702914\n",
      "Step 357, Cost: 1.0239923873256593\n",
      "Step 358, Cost: 1.0231510538541768\n",
      "Step 359, Cost: 1.0223125808685636\n",
      "Step 360, Cost: 1.0214769593292896\n",
      "Step 361, Cost: 1.0206441802442079\n",
      "Step 362, Cost: 1.0198142346681924\n",
      "Step 363, Cost: 1.0189871137027644\n",
      "Step 364, Cost: 1.0181628084957504\n",
      "Step 365, Cost: 1.0173413102409126\n",
      "Step 366, Cost: 1.016522610177608\n",
      "Step 367, Cost: 1.0157066995904336\n",
      "Step 368, Cost: 1.0148935698088886\n",
      "Step 369, Cost: 1.014083212207027\n",
      "Step 370, Cost: 1.013275618203128\n",
      "Step 371, Cost: 1.012470779259357\n",
      "Step 372, Cost: 1.0116686868814317\n",
      "Step 373, Cost: 1.0108693326183025\n",
      "Step 374, Cost: 1.0100727080618215\n",
      "Step 375, Cost: 1.0092788048464305\n",
      "Step 376, Cost: 1.0084876146488286\n",
      "Step 377, Cost: 1.0076991291876705\n",
      "Step 378, Cost: 1.00691334022325\n",
      "Step 379, Cost: 1.0061302395571918\n",
      "Step 380, Cost: 1.005349819032144\n",
      "Step 381, Cost: 1.004572070531476\n",
      "Step 382, Cost: 1.0037969859789855\n",
      "Step 383, Cost: 1.0030245573385896\n",
      "Step 384, Cost: 1.002254776614042\n",
      "Step 385, Cost: 1.0014876358486342\n",
      "Step 386, Cost: 1.0007231271249133\n",
      "Step 387, Cost: 0.999961242564393\n",
      "Step 388, Cost: 0.9992019743272718\n",
      "Step 389, Cost: 0.9984453146121541\n",
      "Step 390, Cost: 0.9976912556557674\n",
      "Step 391, Cost: 0.9969397897326987\n",
      "Step 392, Cost: 0.9961909091551104\n",
      "Step 393, Cost: 0.9954446062724831\n",
      "Step 394, Cost: 0.9947008734713372\n",
      "Step 395, Cost: 0.9939597031749743\n",
      "Step 396, Cost: 0.9932210878432176\n",
      "Step 397, Cost: 0.9924850199721493\n",
      "Step 398, Cost: 0.9917514920938552\n",
      "Step 399, Cost: 0.9910204967761669\n",
      "Step 400, Cost: 0.990292026622418\n",
      "Step 401, Cost: 0.9895660742711865\n",
      "Step 402, Cost: 0.9888426323960543\n",
      "Step 403, Cost: 0.9881216937053525\n",
      "Step 404, Cost: 0.9874032509419318\n",
      "Step 405, Cost: 0.9866872968829126\n",
      "Step 406, Cost: 0.9859738243394526\n",
      "Step 407, Cost: 0.9852628261565051\n",
      "Step 408, Cost: 0.9845542952125925\n",
      "Step 409, Cost: 0.9838482244195714\n",
      "Step 410, Cost: 0.983144606722401\n",
      "Step 411, Cost: 0.982443435098923\n",
      "Step 412, Cost: 0.9817447025596274\n",
      "Step 413, Cost: 0.9810484021474406\n",
      "Step 414, Cost: 0.9803545269374926\n",
      "Step 415, Cost: 0.9796630700369097\n",
      "Step 416, Cost: 0.9789740245845875\n",
      "Step 417, Cost: 0.9782873837509876\n",
      "Step 418, Cost: 0.9776031407379121\n",
      "Step 419, Cost: 0.9769212887783054\n",
      "Step 420, Cost: 0.9762418211360337\n",
      "Step 421, Cost: 0.9755647311056871\n",
      "Step 422, Cost: 0.9748900120123694\n",
      "Step 423, Cost: 0.974217657211501\n",
      "Step 424, Cost: 0.9735476600886083\n",
      "Step 425, Cost: 0.9728800140591322\n",
      "Step 426, Cost: 0.972214712568227\n",
      "Step 427, Cost: 0.9715517490905661\n",
      "Step 428, Cost: 0.9708911171301453\n",
      "Step 429, Cost: 0.9702328102200951\n",
      "Step 430, Cost: 0.9695768219224878\n",
      "Step 431, Cost: 0.9689231458281451\n",
      "Step 432, Cost: 0.9682717755564573\n",
      "Step 433, Cost: 0.9676227047551943\n",
      "Step 434, Cost: 0.9669759271003213\n",
      "Step 435, Cost: 0.9663314362958207\n",
      "Step 436, Cost: 0.9656892260735064\n",
      "Step 437, Cost: 0.9650492901928472\n",
      "Step 438, Cost: 0.9644116224407929\n",
      "Step 439, Cost: 0.9637762166315924\n",
      "Step 440, Cost: 0.9631430666066251\n",
      "Step 441, Cost: 0.9625121662342238\n",
      "Step 442, Cost: 0.9618835094095068\n",
      "Step 443, Cost: 0.9612570900542112\n",
      "Step 444, Cost: 0.9606329021165141\n",
      "Step 445, Cost: 0.9600109395708817\n",
      "Step 446, Cost: 0.959391196417887\n",
      "Step 447, Cost: 0.9587736666840656\n",
      "Step 448, Cost: 0.9581583444217314\n",
      "Step 449, Cost: 0.9575452237088374\n",
      "Step 450, Cost: 0.9569342986488013\n",
      "Step 451, Cost: 0.9563255633703555\n",
      "Step 452, Cost: 0.9557190120273863\n",
      "Step 453, Cost: 0.9551146387987828\n",
      "Step 454, Cost: 0.9545124378882782\n",
      "Step 455, Cost: 0.953912403524301\n",
      "Step 456, Cost: 0.9533145299598242\n",
      "Step 457, Cost: 0.952718811472213\n",
      "Step 458, Cost: 0.9521252423630784\n",
      "Step 459, Cost: 0.9515338169581259\n",
      "Step 460, Cost: 0.9509445296070179\n",
      "Step 461, Cost: 0.9503573746832203\n",
      "Step 462, Cost: 0.949772346583864\n",
      "Step 463, Cost: 0.9491894397295997\n",
      "Step 464, Cost: 0.9486086485644614\n",
      "Step 465, Cost: 0.9480299675557212\n",
      "Step 466, Cost: 0.9474533911937537\n",
      "Step 467, Cost: 0.9468789139919\n",
      "Step 468, Cost: 0.9463065304863288\n",
      "Step 469, Cost: 0.9457362352358999\n",
      "Step 470, Cost: 0.945168022822038\n",
      "Step 471, Cost: 0.9446018878485877\n",
      "Step 472, Cost: 0.9440378249416953\n",
      "Step 473, Cost: 0.9434758287496678\n",
      "Step 474, Cost: 0.9429158939428494\n",
      "Step 475, Cost: 0.9423580152134884\n",
      "Step 476, Cost: 0.9418021872756168\n",
      "Step 477, Cost: 0.9412484048649176\n",
      "Step 478, Cost: 0.9406966627386019\n",
      "Step 479, Cost: 0.9401469556752874\n",
      "Step 480, Cost: 0.9395992784748693\n",
      "Step 481, Cost: 0.9390536259584059\n",
      "Step 482, Cost: 0.9385099929679933\n",
      "Step 483, Cost: 0.9379683743666432\n",
      "Step 484, Cost: 0.9374287650381703\n",
      "Step 485, Cost: 0.9368911598870681\n",
      "Step 486, Cost: 0.9363555538383964\n",
      "Step 487, Cost: 0.9358219418376619\n",
      "Step 488, Cost: 0.9352903188507043\n",
      "Step 489, Cost: 0.9347606798635874\n",
      "Step 490, Cost: 0.9342330198824738\n",
      "Step 491, Cost: 0.9337073339335256\n",
      "Step 492, Cost: 0.9331836170627847\n",
      "Step 493, Cost: 0.932661864336065\n",
      "Step 494, Cost: 0.9321420708388451\n",
      "Step 495, Cost: 0.931624231676157\n",
      "Step 496, Cost: 0.9311083419724769\n",
      "Step 497, Cost: 0.9305943968716226\n",
      "Step 498, Cost: 0.9300823915366455\n",
      "Step 499, Cost: 0.9295723211497237\n",
      "Step 500, Cost: 0.9290641809120606\n",
      "Step 501, Cost: 0.9285579660437809\n",
      "Step 502, Cost: 0.9280536717838272\n",
      "Step 503, Cost: 0.9275512933898559\n",
      "Step 504, Cost: 0.9270508261381442\n",
      "Step 505, Cost: 0.9265522653234806\n",
      "Step 506, Cost: 0.9260556062590695\n",
      "Step 507, Cost: 0.9255608442764359\n",
      "Step 508, Cost: 0.9250679747253223\n",
      "Step 509, Cost: 0.9245769929735951\n",
      "Step 510, Cost: 0.9240878944071468\n",
      "Step 511, Cost: 0.9236006744298033\n",
      "Step 512, Cost: 0.9231153284632216\n",
      "Step 513, Cost: 0.922631851946811\n",
      "Step 514, Cost: 0.9221502403376203\n",
      "Step 515, Cost: 0.9216704891102643\n",
      "Step 516, Cost: 0.9211925937568166\n",
      "Step 517, Cost: 0.9207165497867313\n",
      "Step 518, Cost: 0.9202423527267422\n",
      "Step 519, Cost: 0.9197699981207802\n",
      "Step 520, Cost: 0.9192994815298831\n",
      "Step 521, Cost: 0.9188307985321063\n",
      "Step 522, Cost: 0.9183639447224334\n",
      "Step 523, Cost: 0.9178989157126951\n",
      "Step 524, Cost: 0.9174357071314788\n",
      "Step 525, Cost: 0.916974314624044\n",
      "Step 526, Cost: 0.9165147338522377\n",
      "Step 527, Cost: 0.9160569604944113\n",
      "Step 528, Cost: 0.9156009902453349\n",
      "Step 529, Cost: 0.9151468188161176\n",
      "Step 530, Cost: 0.914694441934123\n",
      "Step 531, Cost: 0.9142438553428887\n",
      "Step 532, Cost: 0.9137950548020437\n",
      "Step 533, Cost: 0.9133480360872327\n",
      "Step 534, Cost: 0.9129027949900309\n",
      "Step 535, Cost: 0.9124593273178677\n",
      "Step 536, Cost: 0.9120176288939488\n",
      "Step 537, Cost: 0.9115776955571778\n",
      "Step 538, Cost: 0.9111395231620779\n",
      "Step 539, Cost: 0.9107031075787158\n",
      "Step 540, Cost: 0.9102684446926282\n",
      "Step 541, Cost: 0.9098355304047416\n",
      "Step 542, Cost: 0.9094043606313014\n",
      "Step 543, Cost: 0.9089749313037966\n",
      "Step 544, Cost: 0.9085472383688817\n",
      "Step 545, Cost: 0.9081212777883133\n",
      "Step 546, Cost: 0.9076970455388659\n",
      "Step 547, Cost: 0.9072745376122687\n",
      "Step 548, Cost: 0.9068537500151294\n",
      "Step 549, Cost: 0.9064346787688632\n",
      "Step 550, Cost: 0.9060173199096259\n",
      "Step 551, Cost: 0.9056016694882394\n",
      "Step 552, Cost: 0.9051877235701247\n",
      "Step 553, Cost: 0.904775478235233\n",
      "Step 554, Cost: 0.9043649295779768\n",
      "Step 555, Cost: 0.9039560737071605\n",
      "Step 556, Cost: 0.9035489067459164\n",
      "Step 557, Cost: 0.903143424831635\n",
      "Step 558, Cost: 0.902739624115897\n",
      "Step 559, Cost: 0.9023375007644143\n",
      "Step 560, Cost: 0.9019370509569565\n",
      "Step 561, Cost: 0.9015382708872898\n",
      "Step 562, Cost: 0.901141156763113\n",
      "Step 563, Cost: 0.9007457048059914\n",
      "Step 564, Cost: 0.9003519112512953\n",
      "Step 565, Cost: 0.8999597723481338\n",
      "Step 566, Cost: 0.8995692843592976\n",
      "Step 567, Cost: 0.8991804435611911\n",
      "Step 568, Cost: 0.8987932462437732\n",
      "Step 569, Cost: 0.8984076887104964\n",
      "Step 570, Cost: 0.8980237672782458\n",
      "Step 571, Cost: 0.897641478277277\n",
      "Step 572, Cost: 0.8972608180511572\n",
      "Step 573, Cost: 0.896881782956708\n",
      "Step 574, Cost: 0.8965043693639425\n",
      "Step 575, Cost: 0.8961285736560081\n",
      "Step 576, Cost: 0.89575439222913\n",
      "Step 577, Cost: 0.8953818214925483\n",
      "Step 578, Cost: 0.8950108578684677\n",
      "Step 579, Cost: 0.8946414977919946\n",
      "Step 580, Cost: 0.894273737711083\n",
      "Step 581, Cost: 0.8939075740864785\n",
      "Step 582, Cost: 0.893543003391661\n",
      "Step 583, Cost: 0.8931800221127902\n",
      "Step 584, Cost: 0.8928186267486508\n",
      "Step 585, Cost: 0.8924588138105977\n",
      "Step 586, Cost: 0.8921005798225007\n",
      "Step 587, Cost: 0.8917439213206916\n",
      "Step 588, Cost: 0.8913888348539118\n",
      "Step 589, Cost: 0.8910353169832552\n",
      "Step 590, Cost: 0.8906833642821212\n",
      "Step 591, Cost: 0.8903329733361565\n",
      "Step 592, Cost: 0.8899841407432078\n",
      "Step 593, Cost: 0.8896368631132667\n",
      "Step 594, Cost: 0.8892911370684214\n",
      "Step 595, Cost: 0.888946959242801\n",
      "Step 596, Cost: 0.8886043262825318\n",
      "Step 597, Cost: 0.8882632348456803\n",
      "Step 598, Cost: 0.8879236816022077\n",
      "Step 599, Cost: 0.8875856632339186\n",
      "Step 600, Cost: 0.8872491764344117\n",
      "Step 601, Cost: 0.886914217909032\n",
      "Step 602, Cost: 0.8865807843748205\n",
      "Step 603, Cost: 0.886248872560467\n",
      "Step 604, Cost: 0.8859184792062625\n",
      "Step 605, Cost: 0.8855896010640514\n",
      "Step 606, Cost: 0.8852622348971841\n",
      "Step 607, Cost: 0.8849363774804693\n",
      "Step 608, Cost: 0.8846120256001293\n",
      "Step 609, Cost: 0.8842891760537519\n",
      "Step 610, Cost: 0.8839678256502449\n",
      "Step 611, Cost: 0.8836479712097922\n",
      "Step 612, Cost: 0.8833296095638034\n",
      "Step 613, Cost: 0.883012737554876\n",
      "Step 614, Cost: 0.8826973520367455\n",
      "Step 615, Cost: 0.8823834498742401\n",
      "Step 616, Cost: 0.8820710279432412\n",
      "Step 617, Cost: 0.8817600831306379\n",
      "Step 618, Cost: 0.8814506123342795\n",
      "Step 619, Cost: 0.8811426124629367\n",
      "Step 620, Cost: 0.8808360804362577\n",
      "Step 621, Cost: 0.880531013184723\n",
      "Step 622, Cost: 0.8802274076496084\n",
      "Step 623, Cost: 0.879925260782936\n",
      "Step 624, Cost: 0.8796245695474365\n",
      "Step 625, Cost: 0.8793253309165071\n",
      "Step 626, Cost: 0.8790275418741703\n",
      "Step 627, Cost: 0.8787311994150315\n",
      "Step 628, Cost: 0.8784363005442393\n",
      "Step 629, Cost: 0.8781428422774445\n",
      "Step 630, Cost: 0.8778508216407607\n",
      "Step 631, Cost: 0.8775602356707233\n",
      "Step 632, Cost: 0.8772710814142495\n",
      "Step 633, Cost: 0.8769833559286001\n",
      "Step 634, Cost: 0.8766970562813395\n",
      "Step 635, Cost: 0.8764121795502969\n",
      "Step 636, Cost: 0.8761287228235282\n",
      "Step 637, Cost: 0.8758466831992743\n",
      "Step 638, Cost: 0.8755660577859278\n",
      "Step 639, Cost: 0.8752868437019931\n",
      "Step 640, Cost: 0.8750090380760439\n",
      "Step 641, Cost: 0.8747326380466951\n",
      "Step 642, Cost: 0.8744576407625578\n",
      "Step 643, Cost: 0.8741840433822061\n",
      "Step 644, Cost: 0.8739118430741373\n",
      "Step 645, Cost: 0.8736410370167376\n",
      "Step 646, Cost: 0.8733716223982458\n",
      "Step 647, Cost: 0.8731035964167181\n",
      "Step 648, Cost: 0.8728369562799878\n",
      "Step 649, Cost: 0.8725716992056354\n",
      "Step 650, Cost: 0.8723078224209477\n",
      "Step 651, Cost: 0.8720453231628891\n",
      "Step 652, Cost: 0.8717841986780589\n",
      "Step 653, Cost: 0.871524446222663\n",
      "Step 654, Cost: 0.8712660630624769\n",
      "Step 655, Cost: 0.8710090464728095\n",
      "Step 656, Cost: 0.8707533937384725\n",
      "Step 657, Cost: 0.8704991021537444\n",
      "Step 658, Cost: 0.8702461690223372\n",
      "Step 659, Cost: 0.8699945916573638\n",
      "Step 660, Cost: 0.8697443673813013\n",
      "Step 661, Cost: 0.8694954935259636\n",
      "Step 662, Cost: 0.8692479674324637\n",
      "Step 663, Cost: 0.8690017864511838\n",
      "Step 664, Cost: 0.8687569479417403\n",
      "Step 665, Cost: 0.868513449272954\n",
      "Step 666, Cost: 0.8682712878228186\n",
      "Step 667, Cost: 0.8680304609784639\n",
      "Step 668, Cost: 0.8677909661361302\n",
      "Step 669, Cost: 0.867552800701133\n",
      "Step 670, Cost: 0.8673159620878335\n",
      "Step 671, Cost: 0.8670804477196058\n",
      "Step 672, Cost: 0.8668462550288083\n",
      "Step 673, Cost: 0.8666133814567503\n",
      "Step 674, Cost: 0.8663818244536643\n",
      "Step 675, Cost: 0.8661515814786735\n",
      "Step 676, Cost: 0.8659226499997609\n",
      "Step 677, Cost: 0.865695027493743\n",
      "Step 678, Cost: 0.865468711446235\n",
      "Step 679, Cost: 0.8652436993516271\n",
      "Step 680, Cost: 0.8650199887130489\n",
      "Step 681, Cost: 0.8647975770423438\n",
      "Step 682, Cost: 0.86457646186004\n",
      "Step 683, Cost: 0.8643566406953186\n",
      "Step 684, Cost: 0.8641381110859885\n",
      "Step 685, Cost: 0.8639208705784557\n",
      "Step 686, Cost: 0.8637049167276933\n",
      "Step 687, Cost: 0.8634902470972183\n",
      "Step 688, Cost: 0.863276859259058\n",
      "Step 689, Cost: 0.8630647507937255\n",
      "Step 690, Cost: 0.86285391929019\n",
      "Step 691, Cost: 0.8626443623458514\n",
      "Step 692, Cost: 0.86243607756651\n",
      "Step 693, Cost: 0.862229062566341\n",
      "Step 694, Cost: 0.8620233149678673\n",
      "Step 695, Cost: 0.8618188324019325\n",
      "Step 696, Cost: 0.8616156125076729\n",
      "Step 697, Cost: 0.8614136529324928\n",
      "Step 698, Cost: 0.8612129513320363\n",
      "Step 699, Cost: 0.8610135053701606\n",
      "Step 700, Cost: 0.8608153127189127\n",
      "Step 701, Cost: 0.8606183710585003\n",
      "Step 702, Cost: 0.8604226780772668\n",
      "Step 703, Cost: 0.8602282314716668\n",
      "Step 704, Cost: 0.8600350289462382\n",
      "Step 705, Cost: 0.8598430682135773\n",
      "Step 706, Cost: 0.8596523469943163\n",
      "Step 707, Cost: 0.8594628630170938\n",
      "Step 708, Cost: 0.8592746140185338\n",
      "Step 709, Cost: 0.8590875977432163\n",
      "Step 710, Cost: 0.8589018119436573\n",
      "Step 711, Cost: 0.8587172543802817\n",
      "Step 712, Cost: 0.8585339228213971\n",
      "Step 713, Cost: 0.8583518150431743\n",
      "Step 714, Cost: 0.8581709288296189\n",
      "Step 715, Cost: 0.8579912619725474\n",
      "Step 716, Cost: 0.857812812271566\n",
      "Step 717, Cost: 0.8576355775340451\n",
      "Step 718, Cost: 0.8574595555750949\n",
      "Step 719, Cost: 0.8572847442175435\n",
      "Step 720, Cost: 0.8571111412919113\n",
      "Step 721, Cost: 0.8569387446363924\n",
      "Step 722, Cost: 0.8567675520968234\n",
      "Step 723, Cost: 0.8565975615266684\n",
      "Step 724, Cost: 0.8564287707869925\n",
      "Step 725, Cost: 0.8562611777464381\n",
      "Step 726, Cost: 0.8560947802812041\n",
      "Step 727, Cost: 0.855929576275024\n",
      "Step 728, Cost: 0.8557655636191394\n",
      "Step 729, Cost: 0.8556027402122831\n",
      "Step 730, Cost: 0.8554411039606538\n",
      "Step 731, Cost: 0.8552806527778943\n",
      "Step 732, Cost: 0.85512138458507\n",
      "Step 733, Cost: 0.8549632973106465\n",
      "Step 734, Cost: 0.85480638889047\n",
      "Step 735, Cost: 0.8546506572677421\n",
      "Step 736, Cost: 0.8544961003930026\n",
      "Step 737, Cost: 0.8543427162241045\n",
      "Step 738, Cost: 0.8541905027261948\n",
      "Step 739, Cost: 0.8540394578716924\n",
      "Step 740, Cost: 0.8538895796402695\n",
      "Step 741, Cost: 0.8537408660188247\n",
      "Step 742, Cost: 0.8535933150014705\n",
      "Step 743, Cost: 0.8534469245895067\n",
      "Step 744, Cost: 0.853301692791401\n",
      "Step 745, Cost: 0.8531576176227701\n",
      "Step 746, Cost: 0.8530146971063585\n",
      "Step 747, Cost: 0.8528729292720172\n",
      "Step 748, Cost: 0.8527323121566853\n",
      "Step 749, Cost: 0.8525928438043684\n",
      "Step 750, Cost: 0.8524545222661208\n",
      "Step 751, Cost: 0.8523173456000237\n",
      "Step 752, Cost: 0.8521813118711663\n",
      "Step 753, Cost: 0.8520464191516254\n",
      "Step 754, Cost: 0.8519126655204474\n",
      "Step 755, Cost: 0.8517800490636277\n",
      "Step 756, Cost: 0.8516485678740924\n",
      "Step 757, Cost: 0.8515182200516777\n",
      "Step 758, Cost: 0.851389003703112\n",
      "Step 759, Cost: 0.8512609169419955\n",
      "Step 760, Cost: 0.851133957888785\n",
      "Step 761, Cost: 0.8510081246707684\n",
      "Step 762, Cost: 0.8508834154220529\n",
      "Step 763, Cost: 0.8507598282835431\n",
      "Step 764, Cost: 0.850637361402921\n",
      "Step 765, Cost: 0.8505160129346315\n",
      "Step 766, Cost: 0.8503957810398614\n",
      "Step 767, Cost: 0.8502766638865202\n",
      "Step 768, Cost: 0.8501586596492254\n",
      "Step 769, Cost: 0.8500417665092821\n",
      "Step 770, Cost: 0.849925982654664\n",
      "Step 771, Cost: 0.8498113062799991\n",
      "Step 772, Cost: 0.8496977355865478\n",
      "Step 773, Cost: 0.8495852687821879\n",
      "Step 774, Cost: 0.8494739040813974\n",
      "Step 775, Cost: 0.849363639705234\n",
      "Step 776, Cost: 0.8492544738813214\n",
      "Step 777, Cost: 0.8491464048438278\n",
      "Step 778, Cost: 0.8490394308334526\n",
      "Step 779, Cost: 0.8489335500974079\n",
      "Step 780, Cost: 0.8488287608893995\n",
      "Step 781, Cost: 0.8487250614696126\n",
      "Step 782, Cost: 0.8486224501046923\n",
      "Step 783, Cost: 0.8485209250677307\n",
      "Step 784, Cost: 0.8484204846382446\n",
      "Step 785, Cost: 0.8483211271021646\n",
      "Step 786, Cost: 0.8482228507518137\n",
      "Step 787, Cost: 0.848125653885894\n",
      "Step 788, Cost: 0.8480295348094692\n",
      "Step 789, Cost: 0.8479344918339475\n",
      "Step 790, Cost: 0.8478405232770659\n",
      "Step 791, Cost: 0.8477476274628758\n",
      "Step 792, Cost: 0.8476558027217227\n",
      "Step 793, Cost: 0.8475650473902339\n",
      "Step 794, Cost: 0.8474753598113018\n",
      "Step 795, Cost: 0.847386738334068\n",
      "Step 796, Cost: 0.8472991813139038\n",
      "Step 797, Cost: 0.8472126871124016\n",
      "Step 798, Cost: 0.8471272540973533\n",
      "Step 799, Cost: 0.8470428806427378\n",
      "Step 800, Cost: 0.8469595651287024\n",
      "Step 801, Cost: 0.8468773059415521\n",
      "Step 802, Cost: 0.846796101473729\n",
      "Step 803, Cost: 0.8467159501238026\n",
      "Step 804, Cost: 0.8466368502964489\n",
      "Step 805, Cost: 0.8465588004024388\n",
      "Step 806, Cost: 0.8464817988586222\n",
      "Step 807, Cost: 0.846405844087913\n",
      "Step 808, Cost: 0.8463309345192745\n",
      "Step 809, Cost: 0.8462570685877033\n",
      "Step 810, Cost: 0.8461842447342159\n",
      "Step 811, Cost: 0.8461124614058337\n",
      "Step 812, Cost: 0.8460417170555675\n",
      "Step 813, Cost: 0.8459720101424036\n",
      "Step 814, Cost: 0.8459033391312898\n",
      "Step 815, Cost: 0.8458357024931195\n",
      "Step 816, Cost: 0.8457690987047187\n",
      "Step 817, Cost: 0.8457035262488312\n",
      "Step 818, Cost: 0.8456389836141037\n",
      "Step 819, Cost: 0.8455754692950723\n",
      "Step 820, Cost: 0.8455129817921494\n",
      "Step 821, Cost: 0.8454515196116071\n",
      "Step 822, Cost: 0.8453910812655655\n",
      "Step 823, Cost: 0.8453316652719775\n",
      "Step 824, Cost: 0.8452732701546138\n",
      "Step 825, Cost: 0.8452158944430549\n",
      "Step 826, Cost: 0.8451595366726685\n",
      "Step 827, Cost: 0.8451041953846034\n",
      "Step 828, Cost: 0.8450498691257701\n",
      "Step 829, Cost: 0.8449965564488335\n",
      "Step 830, Cost: 0.8449442559121931\n",
      "Step 831, Cost: 0.8448929660799733\n",
      "Step 832, Cost: 0.8448426855220091\n",
      "Step 833, Cost: 0.8447934128138328\n",
      "Step 834, Cost: 0.844745146536661\n",
      "Step 835, Cost: 0.8446978852773805\n",
      "Step 836, Cost: 0.8446516276285361\n",
      "Step 837, Cost: 0.8446063721883174\n",
      "Step 838, Cost: 0.8445621175605458\n",
      "Step 839, Cost: 0.8445188623546607\n",
      "Step 840, Cost: 0.8444766051857057\n",
      "Step 841, Cost: 0.8444353446743209\n",
      "Step 842, Cost: 0.8443950794467235\n",
      "Step 843, Cost: 0.8443558081346995\n",
      "Step 844, Cost: 0.8443175293755881\n",
      "Step 845, Cost: 0.8442802418122707\n",
      "Step 846, Cost: 0.8442439440931602\n",
      "Step 847, Cost: 0.8442086348721829\n",
      "Step 848, Cost: 0.8441743128087723\n",
      "Step 849, Cost: 0.8441409765678534\n",
      "Step 850, Cost: 0.8441086248198278\n",
      "Step 851, Cost: 0.8440772562405693\n",
      "Step 852, Cost: 0.8440468695114025\n",
      "Step 853, Cost: 0.8440174633190972\n",
      "Step 854, Cost: 0.8439890363558533\n",
      "Step 855, Cost: 0.843961587319288\n",
      "Step 856, Cost: 0.8439351149124263\n",
      "Step 857, Cost: 0.8439096178436879\n",
      "Step 858, Cost: 0.843885094826874\n",
      "Step 859, Cost: 0.8438615445811569\n",
      "Step 860, Cost: 0.8438389658310675\n",
      "Step 861, Cost: 0.8438173573064832\n",
      "Step 862, Cost: 0.8437967177426178\n",
      "Step 863, Cost: 0.843777045880008\n",
      "Step 864, Cost: 0.8437583404645014\n",
      "Step 865, Cost: 0.8437406002472466\n",
      "Step 866, Cost: 0.8437238239846816\n",
      "Step 867, Cost: 0.8437080104385202\n",
      "Step 868, Cost: 0.8436931583757423\n",
      "Step 869, Cost: 0.8436792665685825\n",
      "Step 870, Cost: 0.8436663337945174\n",
      "Step 871, Cost: 0.8436543588362553\n",
      "Step 872, Cost: 0.8436433404817254\n",
      "Step 873, Cost: 0.8436332775240649\n",
      "Step 874, Cost: 0.8436241687616101\n",
      "Step 875, Cost: 0.8436160129978827\n",
      "Step 876, Cost: 0.8436088090415802\n",
      "Step 877, Cost: 0.8436025557065646\n",
      "Step 878, Cost: 0.843597251811852\n",
      "Step 879, Cost: 0.8435928961816005\n",
      "Step 880, Cost: 0.8435894876450986\n",
      "Step 881, Cost: 0.843587025036758\n",
      "Step 882, Cost: 0.8435855071960973\n",
      "Step 883, Cost: 0.8435849329677365\n",
      "Step 884, Cost: 0.843585301201383\n",
      "Step 885, Cost: 0.8435866107518211\n",
      "Step 886, Cost: 0.8435888604789019\n",
      "Step 887, Cost: 0.8435920492475345\n",
      "Step 888, Cost: 0.8435961759276716\n",
      "Step 889, Cost: 0.8436012393943018\n",
      "Step 890, Cost: 0.843607238527438\n",
      "Step 891, Cost: 0.8436141722121075\n",
      "Step 892, Cost: 0.8436220393383406\n",
      "Step 893, Cost: 0.8436308388011614\n",
      "Step 894, Cost: 0.8436405695005773\n",
      "Step 895, Cost: 0.8436512303415672\n",
      "Step 896, Cost: 0.8436628202340724\n",
      "Step 897, Cost: 0.8436753380929872\n",
      "Step 898, Cost: 0.8436887828381469\n",
      "Step 899, Cost: 0.8437031533943188\n",
      "Step 900, Cost: 0.8437184486911911\n",
      "Step 901, Cost: 0.8437346676633648\n",
      "Step 902, Cost: 0.8437518092503409\n",
      "Step 903, Cost: 0.8437698723965121\n",
      "Step 904, Cost: 0.8437888560511543\n",
      "Step 905, Cost: 0.8438087591684111\n",
      "Step 906, Cost: 0.8438295807072911\n",
      "Step 907, Cost: 0.8438513196316537\n",
      "Step 908, Cost: 0.8438739749101999\n",
      "Step 909, Cost: 0.8438975455164617\n",
      "Step 910, Cost: 0.8439220304287954\n",
      "Step 911, Cost: 0.8439474286303681\n",
      "Step 912, Cost: 0.8439737391091506\n",
      "Step 913, Cost: 0.8440009608579071\n",
      "Step 914, Cost: 0.8440290928741858\n",
      "Step 915, Cost: 0.8440581341603066\n",
      "Step 916, Cost: 0.8440880837233556\n",
      "Step 917, Cost: 0.8441189405751746\n",
      "Step 918, Cost: 0.8441507037323491\n",
      "Step 919, Cost: 0.8441833722162008\n",
      "Step 920, Cost: 0.8442169450527791\n",
      "Step 921, Cost: 0.8442514212728487\n",
      "Step 922, Cost: 0.8442867999118837\n",
      "Step 923, Cost: 0.8443230800100563\n",
      "Step 924, Cost: 0.844360260612226\n",
      "Step 925, Cost: 0.8443983407679351\n",
      "Step 926, Cost: 0.8444373195313939\n",
      "Step 927, Cost: 0.8444771959614749\n",
      "Step 928, Cost: 0.8445179691217044\n",
      "Step 929, Cost: 0.8445596380802489\n",
      "Step 930, Cost: 0.8446022019099112\n",
      "Step 931, Cost: 0.844645659688118\n",
      "Step 932, Cost: 0.8446900104969114\n",
      "Step 933, Cost: 0.8447352534229408\n",
      "Step 934, Cost: 0.8447813875574539\n",
      "Step 935, Cost: 0.8448284119962856\n",
      "Step 936, Cost: 0.8448763258398523\n",
      "Step 937, Cost: 0.8449251281931403\n",
      "Step 938, Cost: 0.8449748181656984\n",
      "Step 939, Cost: 0.8450253948716278\n",
      "Step 940, Cost: 0.845076857429576\n",
      "Step 941, Cost: 0.8451292049627237\n",
      "Step 942, Cost: 0.8451824365987796\n",
      "Step 943, Cost: 0.8452365514699699\n",
      "Step 944, Cost: 0.845291548713031\n",
      "Step 945, Cost: 0.8453474274691993\n",
      "Step 946, Cost: 0.8454041868842026\n",
      "Step 947, Cost: 0.8454618261082527\n",
      "Step 948, Cost: 0.8455203442960363\n",
      "Step 949, Cost: 0.8455797406067055\n",
      "Step 950, Cost: 0.8456400142038704\n",
      "Step 951, Cost: 0.8457011642555888\n",
      "Step 952, Cost: 0.8457631899343606\n",
      "Step 953, Cost: 0.8458260904171178\n",
      "Step 954, Cost: 0.8458898648852132\n",
      "Step 955, Cost: 0.8459545125244189\n",
      "Step 956, Cost: 0.8460200325249098\n",
      "Step 957, Cost: 0.84608642408126\n",
      "Step 958, Cost: 0.8461536863924346\n",
      "Step 959, Cost: 0.8462218186617803\n",
      "Step 960, Cost: 0.8462908200970151\n",
      "Step 961, Cost: 0.8463606899102225\n",
      "Step 962, Cost: 0.8464314273178447\n",
      "Step 963, Cost: 0.8465030315406693\n",
      "Step 964, Cost: 0.8465755018038257\n",
      "Step 965, Cost: 0.8466488373367745\n",
      "Step 966, Cost: 0.8467230373733002\n",
      "Step 967, Cost: 0.8467981011515037\n",
      "Step 968, Cost: 0.8468740279137913\n",
      "Step 969, Cost: 0.8469508169068698\n",
      "Step 970, Cost: 0.847028467381738\n",
      "Step 971, Cost: 0.8471069785936759\n",
      "Step 972, Cost: 0.8471863498022396\n",
      "Step 973, Cost: 0.8472665802712509\n",
      "Step 974, Cost: 0.8473476692687929\n",
      "Step 975, Cost: 0.8474296160671976\n",
      "Step 976, Cost: 0.8475124199430402\n",
      "Step 977, Cost: 0.8475960801771316\n",
      "Step 978, Cost: 0.8476805960545087\n",
      "Step 979, Cost: 0.8477659668644282\n",
      "Step 980, Cost: 0.8478521919003581\n",
      "Step 981, Cost: 0.8479392704599692\n",
      "Step 982, Cost: 0.8480272018451275\n",
      "Step 983, Cost: 0.848115985361887\n",
      "Step 984, Cost: 0.8482056203204813\n",
      "Step 985, Cost: 0.848296106035316\n",
      "Step 986, Cost: 0.8483874418249614\n",
      "Step 987, Cost: 0.8484796270121423\n",
      "Step 988, Cost: 0.8485726609237335\n",
      "Step 989, Cost: 0.8486665428907498\n",
      "Step 990, Cost: 0.8487612722483403\n",
      "Step 991, Cost: 0.8488568483357781\n",
      "Step 992, Cost: 0.8489532704964543\n",
      "Step 993, Cost: 0.8490505380778701\n",
      "Step 994, Cost: 0.8491486504316297\n",
      "Step 995, Cost: 0.8492476069134298\n",
      "Step 996, Cost: 0.8493474068830569\n",
      "Step 997, Cost: 0.8494480497043742\n",
      "Step 998, Cost: 0.8495495347453189\n",
      "Step 999, Cost: 0.8496518613778908\n",
      "Optimized weights: [-0.2445396   0.74736698  0.41342321]\n"
     ]
    }
   ],
   "source": [
    "#normalize\n",
    "X = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 1]])\n",
    "X_norm = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "X_norm[X_norm == 0] = 1  # Prevent division by zero\n",
    "X = X / X_norm\n",
    "\n",
    "#initialize weights\n",
    "w_init = np.random.uniform(-0.1, 0.1, size=3)  # Small initial weights\n",
    "\n",
    "#train\n",
    "w_optimal = train_quantum_logistic_regression_with_cost(X, y, w_init, lr=0.001, epsilon=1e-6, max_steps=1000)\n",
    "print(\"Optimized weights:\", w_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1979a47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data (X): [[0.         0.         0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.57735027 0.57735027 0.57735027]]\n",
      "Labels (y): [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Data (X):\", X)\n",
    "print(\"Labels (y):\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f019e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1, 0, 1, 0]\n",
      "Ground Truth: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def predict(x, w):\n",
    "    \"\"\"Make predictions using the trained weights.\"\"\"\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    x = x / norm_x if norm_x != 0 else x  \n",
    "    prob = compute_gradient_amplitude(x, w)[0]  \n",
    "    return 1 if prob >= 0.5 else 0\n",
    "\n",
    "predictions = [predict(x, w_optimal) for x in X]\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Ground Truth:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d3dd4644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean([predict(x, w_optimal) == label for x, label in zip(X, y)])\n",
    "print(f\"Training Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2985e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "220849cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 1]])\n",
    "# y = np.array([0, 1, 1, 0])  \n",
    "X = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "y = np.array([0, 1, 1, 0, 1, 0, 1])  \n",
    "X_norm = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "X_norm[X_norm == 0] = 1  \n",
    "X = X / X_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51d2b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "n_qubits = 3\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "def feature_map(x):\n",
    "    for i in range(len(x)):\n",
    "        qml.RX(np.pi * x[i], wires=i)\n",
    "    qml.CZ(wires=[0, 1])\n",
    "    qml.CZ(wires=[1, 2])\n",
    "\n",
    "def variational_circuit(params):\n",
    "    num_layers = 3\n",
    "    for l in range(num_layers):\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(params[l, i], wires=i)\n",
    "            qml.RZ(params[l, i + n_qubits], wires=i)\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(x, params):\n",
    "    feature_map(x)\n",
    "    variational_circuit(params)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0291be58",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type ArrayBox which has no callable log method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'ArrayBox' object has no attribute 'log'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m---> 37\u001b[0m     params \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28;01mlambda\u001b[39;00m p: cost(p, X, y), params)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     39\u001b[0m         current_cost \u001b[38;5;241m=\u001b[39m cost(params, X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:93\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.step\u001b[1;34m(self, objective_fn, grad_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_fn, \u001b[38;5;241m*\u001b[39margs, grad_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update trainable arguments with one step of the optimizer.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m        If single arg is provided, list [array] is replaced by array.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     g, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_grad(objective_fn, args, kwargs, grad_fn\u001b[38;5;241m=\u001b[39mgrad_fn)\n\u001b[0;32m     94\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_grad(g, args)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# unwrap from list if one argument, cleaner return\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:122\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.compute_grad\u001b[1;34m(objective_fn, args, kwargs, grad_fn)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute gradient of the objective function at the given point and return it along with\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03mthe objective function forward pass (if available).\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    will not be evaluted and instead ``None`` will be returned.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m g \u001b[38;5;241m=\u001b[39m get_gradient(objective_fn) \u001b[38;5;28;01mif\u001b[39;00m grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_fn\n\u001b[1;32m--> 122\u001b[0m grad \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    123\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    125\u001b[0m num_trainable_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pennylane\\_grad.py:166\u001b[0m, in \u001b[0;36mgrad.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n\u001b[1;32m--> 166\u001b[0m grad_value, ans \u001b[38;5;241m=\u001b[39m grad_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m ans\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autograd\\wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unary_operator(unary_f, x, \u001b[38;5;241m*\u001b[39mnary_op_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnary_op_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pennylane\\_grad.py:184\u001b[0m, in \u001b[0;36mgrad._grad_with_forward\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;129m@unary_to_nary\u001b[39m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_grad_with_forward\u001b[39m(fun, x):\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function is a replica of ``autograd.grad``, with the only\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    difference being that it returns both the gradient *and* the forward pass\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    value.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m     vjp, ans \u001b[38;5;241m=\u001b[39m _make_vjp(fun, x)  \u001b[38;5;66;03m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autograd\\core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_vjp\u001b[39m(fun, x):\n\u001b[0;32m      9\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m VJPNode\u001b[38;5;241m.\u001b[39mnew_root()\n\u001b[1;32m---> 10\u001b[0m     end_value, end_node \u001b[38;5;241m=\u001b[39m  trace(start_node, fun, x)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(g): \u001b[38;5;28;01mreturn\u001b[39;00m vspace(x)\u001b[38;5;241m.\u001b[39mzeros()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autograd\\tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(start_node, fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_stack\u001b[38;5;241m.\u001b[39mnew_trace() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m      9\u001b[0m     start_box \u001b[38;5;241m=\u001b[39m new_box(x, t, start_node)\n\u001b[1;32m---> 10\u001b[0m     end_box \u001b[38;5;241m=\u001b[39m fun(start_box)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isbox(end_box) \u001b[38;5;129;01mand\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_trace \u001b[38;5;241m==\u001b[39m start_box\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_value, end_box\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autograd\\wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     subargs \u001b[38;5;241m=\u001b[39m subvals(args, \u001b[38;5;28mzip\u001b[39m(argnum, x))\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39msubargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[87], line 37\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     35\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m---> 37\u001b[0m     params \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28;01mlambda\u001b[39;00m p: cost(p, X, y), params)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     39\u001b[0m         current_cost \u001b[38;5;241m=\u001b[39m cost(params, X, y)\n",
      "Cell \u001b[1;32mIn[87], line 17\u001b[0m, in \u001b[0;36mcost\u001b[1;34m(params, X, y)\u001b[0m\n\u001b[0;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [(circuit(xi, params) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m X]  \u001b[38;5;66;03m# Map output to [0, 1]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mclip(predictions, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)  \u001b[38;5;66;03m# Avoid log(0)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mqml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mmean(y \u001b[38;5;241m*\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(predictions) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m predictions))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autoray\\autoray.py:81\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m backend \u001b[38;5;241m=\u001b[39m _choose_backend(fn, args, kwargs, like\u001b[38;5;241m=\u001b[39mlike)\n\u001b[0;32m     80\u001b[0m func \u001b[38;5;241m=\u001b[39m get_lib_fn(backend, fn)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type ArrayBox which has no callable log method"
     ]
    }
   ],
   "source": [
    "def cost(params, X, y):\n",
    "    loss = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        pred = (circuit(xi, params) + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
    "        loss += (pred - yi) ** 2\n",
    "    return loss / len(X)\n",
    "params = np.random.uniform(0, 2 * np.pi, size=(2 * n_qubits,))\n",
    "\n",
    "opt = qml.GradientDescentOptimizer(stepsize=0.1)\n",
    "\n",
    "max_steps = 500\n",
    "for step in range(max_steps):\n",
    "    params = opt.step(lambda p: cost(p, X, y), params)\n",
    "    if step % 10 == 0:\n",
    "        current_cost = cost(params, X, y)\n",
    "        print(f\"Step {step}, Cost: {current_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "049bf57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0. 0. 0.], Predicted: 0.341, Actual: 0\n",
      "Input: [0. 1. 0.], Predicted: 0.656, Actual: 1\n",
      "Input: [1. 0. 0.], Predicted: 0.623, Actual: 1\n",
      "Input: [0.57735027 0.57735027 0.57735027], Predicted: 0.272, Actual: 0\n",
      "Input: [0. 0. 1.], Predicted: 0.645, Actual: 1\n",
      "Input: [0.70710678 0.         0.70710678], Predicted: 0.629, Actual: 0\n",
      "Input: [0.         0.70710678 0.70710678], Predicted: 0.441, Actual: 1\n"
     ]
    }
   ],
   "source": [
    "for xi, yi in zip(X, y):\n",
    "    pred = (circuit(xi, params) + 1) / 2\n",
    "    print(f\"Input: {xi}, Predicted: {pred:.3f}, Actual: {yi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733573f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c573f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Cost: 0.58018815930157\n",
      "Step 50, Cost: 0.3391633350827611\n",
      "Step 100, Cost: 0.314622631512258\n",
      "Step 150, Cost: 0.2623522719496512\n",
      "Step 200, Cost: 0.208048122768597\n",
      "Step 250, Cost: 0.17490883080219294\n",
      "Step 300, Cost: 0.17418483774884377\n",
      "Step 350, Cost: 0.17416668702107216\n",
      "Step 400, Cost: 0.17416652160529233\n",
      "Step 450, Cost: 0.17416652133375382\n",
      "Step 500, Cost: 0.17416652133330554\n",
      "Step 550, Cost: 0.17416652133330485\n",
      "Step 600, Cost: 0.17416652133330474\n",
      "Step 650, Cost: 0.17416652133330487\n",
      "Step 700, Cost: 0.17416652133330482\n",
      "Step 750, Cost: 0.17416652133330487\n",
      "Step 800, Cost: 0.17416652133330454\n",
      "Step 850, Cost: 0.17416666260040528\n",
      "Step 900, Cost: 0.17416652392601967\n",
      "Step 950, Cost: 0.17416652136383926\n",
      "\n",
      "Testing the Model:\n",
      "Input: [0. 0. 0.], Predicted Probability: 0.069, Predicted Label: 0, Actual: 0\n",
      "Input: [0. 1. 0.], Predicted Probability: 0.873, Predicted Label: 1, Actual: 1\n",
      "Input: [1. 0. 0.], Predicted Probability: 0.914, Predicted Label: 1, Actual: 1\n",
      "Input: [0.57735027 0.57735027 0.57735027], Predicted Probability: 0.181, Predicted Label: 0, Actual: 0\n",
      "Input: [0. 0. 1.], Predicted Probability: 0.900, Predicted Label: 1, Actual: 1\n",
      "Input: [0.70710678 0.         0.70710678], Predicted Probability: 0.231, Predicted Label: 0, Actual: 0\n",
      "Input: [0.         0.70710678 0.70710678], Predicted Probability: 0.702, Predicted Label: 1, Actual: 1\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np  \n",
    "\n",
    "n_qubits = 3\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "def feature_map(x):\n",
    "    for i in range(len(x)):\n",
    "        qml.RX(np.pi * x[i], wires=i)\n",
    "        qml.RZ(np.pi * x[i], wires=i)\n",
    "    for i in range(n_qubits):\n",
    "        qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "\n",
    "def variational_circuit(params):\n",
    "    num_layers = params.shape[0]\n",
    "    for l in range(num_layers):\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(params[l, i], wires=i)\n",
    "            qml.RZ(params[l, i + n_qubits], wires=i)\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "\n",
    "@qml.qnode(dev, interface='autograd')\n",
    "def circuit(x, params):\n",
    "    feature_map(x)\n",
    "    variational_circuit(params)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def cost(params, X, y):\n",
    "    predictions = [(circuit(xi, params) + 1) / 2 for xi in X]\n",
    "    predictions = qml.numpy.stack(predictions)\n",
    "    predictions = qml.numpy.clip(predictions, 1e-6, 1 - 1e-6)\n",
    "    y = qml.numpy.array(y)\n",
    "    loss = -qml.numpy.mean(y * qml.numpy.log(predictions) + (1 - y) * qml.numpy.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "# Input \n",
    "X = qml.numpy.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "\n",
    "y = qml.numpy.array([0, 1, 1, 0, 1, 0, 1])  \n",
    "\n",
    "#normalize \n",
    "X_norm = qml.numpy.linalg.norm(X, axis=1, keepdims=True)\n",
    "X_norm[X_norm == 0] = 1  \n",
    "X = X / X_norm\n",
    "\n",
    "#initialize \n",
    "num_layers = 3\n",
    "params = qml.numpy.random.uniform(0, 2 * qml.numpy.pi, size=(num_layers, 2 * n_qubits))\n",
    "\n",
    "#set up an Adam optimizer with a smaller learning rate\n",
    "opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "#training Loop\n",
    "max_steps = 1000\n",
    "for step in range(max_steps):\n",
    "    params = opt.step(lambda p: cost(p, X, y), params)\n",
    "    if step % 50 == 0:\n",
    "        current_cost = cost(params, X, y)\n",
    "        print(f\"Step {step}, Cost: {current_cost}\")\n",
    "\n",
    "#testing the Model\n",
    "print(\"\\nTesting the Model:\")\n",
    "for xi, yi in zip(X, y):\n",
    "    pred = (circuit(xi, params) + 1) / 2\n",
    "    predicted_label = int(pred > 0.5)\n",
    "    print(f\"Input: {xi}, Predicted Probability: {pred:.3f}, Predicted Label: {predicted_label}, Actual: {yi}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa31c5db",
   "metadata": {},
   "source": [
    "A part of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6335ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Cost: 0.6932500939008935\n",
      "Step 10, Cost: 0.35715124203645565\n",
      "Step 20, Cost: 0.18505877173700128\n",
      "Step 30, Cost: 0.12112138911981549\n",
      "Step 40, Cost: 0.10966481788669509\n",
      "\n",
      "Model accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv('PS_20174392719_1491204439457_log.csv')\n",
    "\n",
    "#subset of the data for computational feasibility\n",
    "data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "features = data[['amount', 'oldbalanceOrg']].values\n",
    "labels = data['isFraud'].values.astype(float)  # Convert labels to float\n",
    "\n",
    "#normalize \n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#quantum circuit parameters\n",
    "n_qubits = features.shape[1]\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "def feature_map(x):\n",
    "    for i in range(len(x)):\n",
    "        qml.RX(x[i], wires=i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "def variational_circuit(params):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(params[i], wires=i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CZ(wires=[i, i + 1])\n",
    "\n",
    "@qml.qnode(dev, interface='autograd')\n",
    "def circuit(x, params):\n",
    "    feature_map(x)\n",
    "    variational_circuit(params)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def cost(params, X, y):\n",
    "    y = np.array(y)  \n",
    "    predictions = [(circuit(xi, params) + 1) / 2 for xi in X]\n",
    "    predictions = np.stack(predictions)  \n",
    "    predictions = np.clip(predictions, 1e-6, 1 - 1e-6)  \n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "\n",
    "params = np.random.uniform(0, 2 * np.pi, size=(n_qubits,))\n",
    "\n",
    "opt = qml.AdamOptimizer(stepsize=0.05)\n",
    "\n",
    "#training loop\n",
    "max_steps = 50\n",
    "for step in range(max_steps):\n",
    "    params = opt.step(lambda p: cost(p, features, labels), params)\n",
    "    if step % 10 == 0:\n",
    "        current_cost = cost(params, features, labels)\n",
    "        print(f\"Step {step}, Cost: {current_cost}\")\n",
    "\n",
    "#evaluate the model\n",
    "predictions = [(circuit(xi, params) + 1) / 2 for xi in features]\n",
    "predicted_labels = (np.array(predictions) > 0.5).astype(int)\n",
    "\n",
    "#calculate accuracy\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "print(f\"\\nModel accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e759fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ead7bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Cost: 0.5064614980225101\n",
      "Step 10, Cost: 0.23389472111059076\n",
      "Step 20, Cost: 0.11067603649221922\n",
      "Step 30, Cost: 0.07888306056502507\n",
      "Step 40, Cost: 0.07795173209772441\n",
      "\n",
      "Model accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('PS_20174392719_1491204439457_log.csv')\n",
    "data = data.sample(n=10000, random_state=42)\n",
    "\n",
    "data = data.drop(['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'], axis=1)\n",
    "\n",
    "type_encoder = OneHotEncoder(sparse_output=False)\n",
    "type_encoded = type_encoder.fit_transform(data[['type']])\n",
    "\n",
    "features = np.hstack((\n",
    "    data[['amount', 'isFlaggedFraud']].values,\n",
    "    type_encoded\n",
    "))\n",
    "labels = data['isFraud'].values.astype(float)  \n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "n_qubits = features.shape[1]\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "def feature_map(x):\n",
    "    for i in range(len(x)):\n",
    "        qml.RX(x[i], wires=i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "def variational_circuit(params):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(params[i], wires=i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CZ(wires=[i, i + 1])\n",
    "\n",
    "@qml.qnode(dev, interface='autograd')\n",
    "def circuit(x, params):\n",
    "    feature_map(x)\n",
    "    variational_circuit(params)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def cost(params, X, y):\n",
    "    y = np.array(y)  \n",
    "    predictions = [(circuit(xi, params) + 1) / 2 for xi in X]\n",
    "    predictions = np.stack(predictions)  \n",
    "    predictions = np.clip(predictions, 1e-6, 1 - 1e-6)  \n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "params = np.random.uniform(0, 2 * np.pi, size=(n_qubits,))\n",
    "\n",
    "opt = qml.AdamOptimizer(stepsize=0.05)\n",
    "\n",
    "max_steps = 50\n",
    "for step in range(max_steps):\n",
    "    params = opt.step(lambda p: cost(p, features, labels), params)\n",
    "    if step % 10 == 0:\n",
    "        current_cost = cost(params, features, labels)\n",
    "        print(f\"Step {step}, Cost: {current_cost}\")\n",
    "\n",
    "predictions = [(circuit(xi, params) + 1) / 2 for xi in features]\n",
    "predicted_labels = (np.array(predictions) > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "print(f\"\\nModel accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1d4dc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.07795173, requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2954e927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76cff85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, ..., 0, 0, 0], requires_grad=True)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3051d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.21684702, 4.15582199, 3.33690872, 3.38518135, 4.08758266,\n",
       "        2.44025071, 3.3761743 ], requires_grad=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d035ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.33572084, -0.0100005 ,  1.86986707, ..., -0.07637955,\n",
       "         -0.71502369, -0.30184015],\n",
       "        [-0.34763888, -0.0100005 , -0.53479737, ..., -0.07637955,\n",
       "          1.39855506, -0.30184015],\n",
       "        [-0.04600445, -0.0100005 ,  1.86986707, ..., -0.07637955,\n",
       "         -0.71502369, -0.30184015],\n",
       "        ...,\n",
       "        [-0.13130636, -0.0100005 , -0.53479737, ..., -0.07637955,\n",
       "         -0.71502369, -0.30184015],\n",
       "        [-0.37142702, -0.0100005 , -0.53479737, ..., -0.07637955,\n",
       "          1.39855506, -0.30184015],\n",
       "        [-0.16550449, -0.0100005 ,  1.86986707, ..., -0.07637955,\n",
       "         -0.71502369, -0.30184015]], requires_grad=True)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8fe74ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a4f0228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CASH_IN', 'PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "554ac00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737323</th>\n",
       "      <td>278</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>330218.42</td>\n",
       "      <td>C632336343</td>\n",
       "      <td>C834976624</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264914</th>\n",
       "      <td>15</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11647.08</td>\n",
       "      <td>C1264712553</td>\n",
       "      <td>M215391829</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85647</th>\n",
       "      <td>10</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>152264.21</td>\n",
       "      <td>C1746846248</td>\n",
       "      <td>C1607284477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899326</th>\n",
       "      <td>403</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>1551760.63</td>\n",
       "      <td>C333676753</td>\n",
       "      <td>C1564353608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544263</th>\n",
       "      <td>206</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>78172.30</td>\n",
       "      <td>C813403091</td>\n",
       "      <td>C1091768874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094229</th>\n",
       "      <td>301</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>80646.44</td>\n",
       "      <td>C819258095</td>\n",
       "      <td>C1433007346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603252</th>\n",
       "      <td>394</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>57501.43</td>\n",
       "      <td>C1316678628</td>\n",
       "      <td>C483888900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6189570</th>\n",
       "      <td>570</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>112497.83</td>\n",
       "      <td>C765932428</td>\n",
       "      <td>C355959262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853779</th>\n",
       "      <td>402</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>557.43</td>\n",
       "      <td>C2108927530</td>\n",
       "      <td>M918034835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615363</th>\n",
       "      <td>156</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>96555.21</td>\n",
       "      <td>C703958752</td>\n",
       "      <td>C115254823</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         step      type      amount     nameOrig     nameDest  isFraud  \\\n",
       "3737323   278   CASH_IN   330218.42   C632336343   C834976624        0   \n",
       "264914     15   PAYMENT    11647.08  C1264712553   M215391829        0   \n",
       "85647      10   CASH_IN   152264.21  C1746846248  C1607284477        0   \n",
       "5899326   403  TRANSFER  1551760.63   C333676753  C1564353608        0   \n",
       "2544263   206   CASH_IN    78172.30   C813403091  C1091768874        0   \n",
       "...       ...       ...         ...          ...          ...      ...   \n",
       "4094229   301  CASH_OUT    80646.44   C819258095  C1433007346        0   \n",
       "5603252   394   CASH_IN    57501.43  C1316678628   C483888900        0   \n",
       "6189570   570  CASH_OUT   112497.83   C765932428   C355959262        0   \n",
       "5853779   402   PAYMENT      557.43  C2108927530   M918034835        0   \n",
       "1615363   156   CASH_IN    96555.21   C703958752   C115254823        0   \n",
       "\n",
       "         isFlaggedFraud  \n",
       "3737323               0  \n",
       "264914                0  \n",
       "85647                 0  \n",
       "5899326               0  \n",
       "2544263               0  \n",
       "...                 ...  \n",
       "4094229               0  \n",
       "5603252               0  \n",
       "6189570               0  \n",
       "5853779               0  \n",
       "1615363               0  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b55fdcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCASH_IN -> 1 0 0 0 0\\nPAYMENT -> 0 0 0 1 0\\nCASH_OUT-> 0 1 0 0 0\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "CASH_IN -> 1 0 0 0 0\n",
    "PAYMENT -> 0 0 0 1 0\n",
    "CASH_OUT-> 0 1 0 0 0\n",
    "TRANSFER-> 0 0 0 0 1\n",
    "DEBIT ->   0 0 1 0 0 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c314f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters have been saved to 'trained_params.npy'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('trained_params_trial1.npy', params)\n",
    "\n",
    "print(\"Model parameters have been saved to 'trained_params.npy'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c47e3e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler and encoder have been saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('scaler_trial1.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('type_encoder_trial1.pkl', 'wb') as f:\n",
    "    pickle.dump(type_encoder, f)\n",
    "\n",
    "print(\"Scaler and encoder have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a47e1a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [quantum_decision_tree_predict(x, params) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_test]\n\u001b[0;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predictions)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = [quantum_decision_tree_predict(x, params) for x in X_test]\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nTest set accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d7cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
